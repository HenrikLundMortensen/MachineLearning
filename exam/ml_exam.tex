\documentclass[oneside]{memoir}
\usepackage[]{amsmath}
\usepackage[]{bm}
\usepackage[]{microtype}

\renewcommand{\vec}[1]{\ensuremath{\bm{#1}}}

\counterwithout{section}{chapter}

\title{Machine learning exam 2017}
\author{Henrik Lund Mortensen}

\begin{document}
\maketitle

\section{Linear Models}
\section{Learning Theory}


\begin{itemize}
\item Supervised learning
\item In-sample error vs out-of-sample error
\end{itemize}

\subsection{Learning feasibility}
In general, we have an unknown function $f(x)$ and only a finte set of samples $\mathcal{D} = \{ x_i,f(x_i)\}$. Our job is to use $\mathcal{D}$ to estimate $f$ outisde of $\mathcal{D}$. Consider a set of hypotheses, $\mathcal{H}$. The goal is to choose the hypothesis, $g \in \mathcal{H}$ that approximates $f(x)$ best. There are two thing we must consider:

\begin{enumerate}
\item Can we find a $g$ such that the training samples are well explained, i.e. low $E_\text{in}(g)$?
\item Can we make sure that the $E_\text{out}(g)$ is close to $E_\text{in}(g)$?
\end{enumerate}
The answer to the first question depends on the complexity of $f$ and the $\mathcal{H}$. It is clear that we can always find a set $\mathcal{H}$ which contains a hypothesis that matches the training data perfectly, i.e. $E_{\text{in}} = 0$. However, we get in trouble in the part 2 above. We cannot guarantee that $E_\text{out}(g)$ is close to $E_\text{in}(g)$, but we can estimate the probability that is it. This is given by the Hoeffding bound
\begin{equation}
  \label{hoeffding}
\mathcal{P}\left[ |E_\text{out}(g) - E_\text{in}(g)| > \epsilon\right] \leq 2M e^{-2\epsilon^2N},
\end{equation}
where $N$ is the number of samples in $\mathcal{D}$ and $M$ is the size of $\mathcal{H}$ (number of hypotheses). There is a trade-off between choosing $\mathcal{H}$ complex enough to describe $f$ well, while low enough to have a reasonable bound in Eq. (\ref{hoeffding}). The Hoeffding bound only applies to finte $\mathcal{H}$, as it becomes meaningless when $M\rightarrow \infty$.

When the hypothesis space becomes infinite we must replace $M$ by something else. We call the replacement the \emph{growth function}. We restrict outselves to binary target functions, such that the target function (and our hypotheses, $h \in \mathcal{H}$) map from $\mathcal{X}$ to $\{+1,-1\}$. Further, we define a dichotomy as an $N$-tuple of $+1$'s and $-1$'s such that the dichotomies generated by $\mathcal{H}$ on some set $x_1,x_2,\dots,x_N \in \mathcal{X}$ is given by

\begin{equation}
  \label{dichotomies}
  \{h(x_1),h(x_2),\dots,h(x_N) | \forall h \in \mathcal{H}\}. 
\end{equation}
It is clear that there can be a maximum of $2^N$ different dichotomies for $N$ points. However, it is not true that any $\mathcal{H}$ can generate all dichotomies. We say that if $\mathcal{H}$ generates all possible dichotomies for some set, it \emph{shatters} this set. 


\section{Support Vector Machines}

In a binary classifier labels inputs either $+1$ or $-1$ (for example). However, points close to the decision boundary are more uncertain that those far from it.  


\section{Neural Nets}
\section{Decision Trees and Ensemble Methods}
\section{Hidden Markov Models - Decoding}
\section{Hidden Markov Models - Training}
\section{Unsupervised Learning - Clustering}

Unsupervised learning is used when data is available, but comes without labels, i.e. there are no training examples. An example of unsupervised learning is clustering. Here data are labeled into groups (or clusters).

\subsection{$K$-means}

The K-means problem is s clustering problem, where the goal is to find $k$ means, $\mu_i$, such that the Euclidian distance to each cluster member is minimized,

\begin{equation}
  \label{SSE k means}
SSE(\mathcal{C}) = \sum_{i=1}^k \sum_{\vec{x}_j\in \mathcal{C}_i} ||\vec{x}_j - \vec{\mu}_i||^2. 
\end{equation}
In general, it is hard to find the optimal $\vec{\mu}_i$. However, an iterative method known as Lloyd's algorithm is often used as an approximation. Here, we start by choosing $k$ random $\vec{\mu}_i$ and determine for all data points which $\vec{\mu}_i$ is closest. The new $\vec{\mu}_i$ are now chosen as the mean of all $\vec{x}_j \in \mathcal{C}_i$. This is repeated untill no data points changes cluster.

Lloyd's algorithm is fast, but can only give convex clusters (draw non-convex example). Also, sometimes a mean is not well-defined.

\subsection{$K$-medoids}

Similar to $K$-means, but here a representative datapoint is chosen as cluster center. The algorithm is initialized by chosing $k$ random points as cluster centers. The cost is defined as
\begin{equation}
  \label{kmedoid cost}
  \text{TD} = \sum_i \sum_{p\in \mathcal{C}_i} \text{dist} (p,x_i).
\end{equation}
Then, for each cluster center, $x_i$, and data point that is not a cluster center, $q$, we swap such that $q$ becomes a cluster center and calculates TD again. The swap with the lowest cost is now chosen. This repeats untill there is no change in TD.

$k$-medoids is sometimes applied instead of $k$-means, e.g. if a mean is ill-defined in the specific problem. It is also not as sensitive to outliers. The implementation cost is $\mathcal{O}(N^2k^2)$ (I think), which makes it much more expensive than $k$-means.

\subsection{Expectation Maximization Clustering}
Unlike $k$-means and $k$-medoids, EM clustering is a \emph{soft clustering} method. A probability for belonging to a certain cluster is assigned to each data point, instead of a hard label.

In EM clustering we find $n$ clusters represented by multivariate normal distributions.
\begin{equation}
  \label{EM clustering f}
  f_i(\vec{x}|\vec{\mu}_i,\vec{\Sigma}_i) = \frac{1}{(2\pi)^d |\vec{\Sigma}_i|^{\frac{1}{2}}}  \exp \left[ - \frac{(\vec{x}-\vec{\mu}_i)^T\vec{\Sigma}_i^{-1}(\vec{x}-\vec{\mu}_i)}{2} \right]. 
\end{equation}
The assumption is the $x$ is generated by a Gaussian mixture model probability distribution defined by
\begin{equation}
  \label{gaussian mixture model}
f(\vec{x}) = \sum_i f_i(\vec{x}) P(\mathcal{C}_i),
\end{equation}
where the mixture parameters satisfies $\sum_i P(\mathcal{C}_i) = 1$. The model parameters thus include $\theta = \{ \vec{\mu},\vec{\Sigma},P(\mathcal{C}_i)\}$. This goal is to find the set of parameters, such that the probability of observing the dataset, $\vec{D}$, is maximized,
\begin{equation}
  \label{theta max}
  \theta^* = \arg \max_{\theta} \; P(\vec{D}|\theta) =  \arg \max_{\theta} \; \ln P(\vec{D}|\theta).
\end{equation}
Finding $\theta^*$ is generally hard. Again, we can use an iterative algorithm to approximate $\theta^*$ in a two-step process. The first step (\emph{estimation step}) is to calculate $w_{ij} = P(\mathcal{C}_i|\vec{x}_j)$. This is done using Bayes rule
\begin{equation}
  \label{bayes EM}
  P(\mathcal{C}_i|x_j) = \frac{P(x_j) P(x_j|\mathcal{C}_i)}{P(\mathcal{C}_i)}.
\end{equation}
Here, $P(\mathcal{C}_i)$ is initialized as $\frac{1}{n}$. $P(x_j|\mathcal{C}_i)$ is easily calculated by Eq. (\ref{EM clustering f}) and $P(x_j) = \sum_i P(x_j|\mathcal{C}_i) P(\mathcal{C}_i)$. 

The second step (\emph{maximization step}) is to reestimate the model parameters using the weights, $w_{ij}$. The means are estimated as
\begin{equation}
  \label{EM means}
  \vec{\mu}_i = \frac{\sum_j w_{ij} \vec{x}_j}{\sum_j w_{ij}}.
\end{equation}
The cluster variance matrix is assumed diagonal for simplicity (otherwise the number of parameters is huge). The covariance matrix is esimated as the weighted covariance for each cluster,
\begin{equation}
  \label{covariance estimate}
  \vec{\Sigma}_i = \frac{\sum_j w_{ij} \vec{z}_{ji}\cdot \vec{z}^T_{ji}}{\vec{w}_i^T\bm{1}},
\end{equation}
where $\vec{z}_{ji} = \vec{x}_j - \vec{\mu}_i$. Finally, the mixture parameters are estimated as
\begin{equation}
  \label{mixture parameter estimation}
  P(\mathcal{C}_i) = \frac{\vec{w}_i^T \vec{1}}{n}.
\end{equation}
These update rules follows from differentiation of the log likelihood function, $\ln P(\vec{D}|\theta)$, with respect to the parameters, and setting it to zero. 

\subsection{DBSCAN}
DBSCAN is a density based clustering method. Instead of considering distances between all data points, only the neighbourhoods of the data points are considered. The idea is to cluster data points based on the local density.

Consider a data point $q$ in the dataset $\mathcal{D}$. Any point, $p$, which is closer to $q$ than some threshold, $\epsilon$, is said to be in the neighbourhood of $q$,
\begin{equation}
  \label{neighbourhood of q}
\mathcal{N}_\epsilon(q) = \left\{\;p\;\;|\; \text{dist}(p,q)\leq \epsilon\; \right\}.
\end{equation}
Some definitions;
\begin{itemize}
\item $q$ is a \emph{core point} of the cluster if $|\mathcal{N}_\epsilon(q)| \geq \text{\emph{MinPts}}$. 
\item $p$ is \emph{directly density reachable} from $q$ if $p \in \mathcal{N}_\epsilon(q)$. 
\item $p$ is \emph{density reachable} from $s$ if there exists a chain of core points such that the chain elements are directly reachable from its chain neighbour.
\item Two points, $p$ and $s$, are \emph{density connected} if they are both density connected to some core point, $q$.
\end{itemize}
The algorithm first computes $N_\epsilon(p)$ for all points $p\in \mathcal{D}$. Based on this, all core points are identified. Next, the density connected clusters are located by going through $N_\epsilon(q)$ for all core points.

The complexity depends on how efficiently the neighbourhood can be determined. Worst case is $\mathcal{O}(N^2)$, but if the dimension is not to high it can be done in $\mathcal{O}(N\log N)$.

An advantage is that no number of clusters is to be specified before hand. Also, non-convex cluster shapes can be found. A disadvantage is that $\epsilon$ is har to choose, especially if some clusters are on different scales.

\subsection{DENCLUE}
The DENCLUE algorithm is also a density based clustering algorithm. 



\subsection{Performance measures}
\dots

\section{Unsupervised Learning - Outlier Detection and Dimensionality Reduction}

\subsection{PCA}
In Principal Component Analisys (PCA), we seek to represent data in a high dimentional space in a low dimensional containing the most important features. Data may be correlated in a way that makes the high dimensional representation essentially redundant. Consider images of faces represented by $100 \text{ pixels}\times 100 \text{ pixels} = 10000$ dimensional space. With PCA, this can be reduced to $\sim20$ dimensions representing the most essential face features. 

The first step in PCA is to normalize the data, such that it has zero mean and unit variance. This is done by
\begin{align}
  x^{(i)} &\rightarrow x^{(i)} - \mu\;,\;\;\; \mu = \frac{1}{m}\sum_{i=1}^m x^{(i)} 
\end{align}
and then
\begin{align}
  x^{(i)}_j &\rightarrow \frac{x^{(i)}_j}{\sigma_j}\;,\;\;\; \sigma_j = \frac{1}{m}\sum_{i=1}^m  \left(x^{(i)}_j\right)^2 .
\end{align}
The goal is now to find a vector $\vec{u}$ such that when the data is projected onto $\vec{u}$ it has the highest variance. That is, we seek to maximize
\begin{equation}
  \label{variance u projection}
  \frac{1}{m} \sum_{i=1}^m \left( \vec{x}^{(i)^T}\vec{u} \right)^2 = \vec{u}^T \left(\frac{1}{m}\sum_{i=1}^m \vec{x}^{(i)}\vec{x}^{(i)^T} \right) \vec{u}.
\end{equation}
Maximizing this quantity with the constraint that $\vec{u}$ is a unit vector reveals that $\vec{u}$ must be an eigenvector of the estimated covariance matrix
\begin{equation}
  \label{estimated covariance matrix}
\Sigma = \frac{1}{m}\sum_{i=1}^m \vec{x}^{(i)}\vec{x}^{(i)^T}.
\end{equation}
To reduce the high dimentional space to a $k$-dimensional space we simply project onto the first $k$ eigenvectors of $\Sigma$,
\begin{equation}
  \label{PCA y}
y^{(i)} = 
\left[  \begin{matrix}
    \vec{u}_1^T \vec{x^{(i)}} \\
    \vec{u}_2^T \vec{x^{(i)}} \\
    \vdots \\
    \vec{u}_k^T \vec{x^{(i)}} 
  \end{matrix}\right].
\end{equation}



\end{document}