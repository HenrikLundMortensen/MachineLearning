\documentclass[oneside]{memoir}
\usepackage[]{amsmath}
\usepackage[]{bm}
\usepackage[]{microtype}

\renewcommand{\vec}[1]{\ensuremath{\bm{#1}}}

\counterwithout{section}{chapter}

\title{Machine learning exam 2017}
\author{Henrik Lund Mortensen}

\begin{document}
\maketitle

\section{Linear Models}
\section{Learning Theory}


\begin{itemize}
\item Supervised learning
\item In-sample error vs out-of-sample error
\end{itemize}

\subsection{Learning feasibility}
In general, we have an unknown function $f(x)$ and only a finte set of samples $\mathcal{D} = \{ x_i,f(x_i)\}$. Our job is to use $\mathcal{D}$ to estimate $f$ outisde of $\mathcal{D}$. Consider a set of hypotheses, $\mathcal{H}$. The goal is to choose the hypothesis, $g \in \mathcal{H}$ that approximates $f(x)$ best. There are two thing we must consider:

\begin{enumerate}
\item Can we find a $g$ such that the training samples are well explained, i.e. low $E_\text{in}(g)$?
\item Can we make sure that the $E_\text{out}(g)$ is close to $E_\text{in}(g)$?
\end{enumerate}
The answer to the first question depends on the complexity of $f$ and the $\mathcal{H}$. It is clear that we can always find a set $\mathcal{H}$ which contains a hypothesis that matches the training data perfectly, i.e. $E_{\text{in}} = 0$. However, we get in trouble in the part 2 above. We cannot guarantee that $E_\text{out}(g)$ is close to $E_\text{in}(g)$, but we can estimate the probability that is it. This is given by the Hoeffding bound
\begin{equation}
  \label{hoeffding}
\mathcal{P}\left[ |E_\text{out}(g) - E_\text{in}(g)| > \epsilon\right] \leq 2M e^{-2\epsilon^2N},
\end{equation}
where $N$ is the number of samples in $\mathcal{D}$ and $M$ is the size of $\mathcal{H}$ (number of hypotheses). There is a trade-off between choosing $\mathcal{H}$ complex enough to describe $f$ well, while low enough to have a reasonable bound in Eq. (\ref{hoeffding}). The Hoeffding bound only applies to finte $\mathcal{H}$, as it becomes meaningless when $M\rightarrow \infty$.

When the hypothesis space becomes infinite we must replace $M$ by something else. We call the replacement the \emph{growth function}. We restrict outselves to binary target functions, such that the target function (and our hypotheses, $h \in \mathcal{H}$) map from $\mathcal{X}$ to $\{+1,-1\}$. Further, we define a dichotomy as an $N$-tuple of $+1$'s and $-1$'s such that the dichotomies generated by $\mathcal{H}$ on some set $x_1,x_2,\dots,x_N \in \mathcal{X}$ is given by

\begin{equation}
  \label{dichotomies}
  \{h(x_1),h(x_2),\dots,h(x_N) | \forall h \in \mathcal{H}\}. 
\end{equation}
It is clear that there can be a maximum of $2^N$ different dichotomies for $N$ points. However, it is not true that any $\mathcal{H}$ can generate all dichotomies. We say that if $\mathcal{H}$ generates all possible dichotomies for some set, it \emph{shatters} this set. 


\section{Support Vector Machines}

In a binary classifier labels inputs either $+1$ or $-1$ (for example). However, points close to the decision boundary are more uncertain that those far from it.  


\section{Neural Nets}
\section{Decision Trees and Ensemble Methods}
\section{Hidden Markov Models - Decoding}
\section{Hidden Markov Models - Training}
\section{Unsupervised Learning - Clustering}

Unsupervised learning is used when data is available, but comes without labels, i.e. there are no training examples. An example of unsupervised learning is clustering. Here data are labeled into groups (or clusters).

\subsection{$K$-means}

The K-means problem is s clustering problem, where the goal is to find $k$ means, $\mu_i$, such that the Euclidian distance to each cluster member is minimized,

\begin{equation}
  \label{SSE k means}
SSE(\mathcal{C}) = \sum_{i=1}^k \sum_{\vec{x}_j\in \mathcal{C}_i} ||\vec{x}_j - \vec{\mu}_i||^2. 
\end{equation}
In general, it is hard to find the optimal $\vec{\mu}_i$. However, an iterative method known as Lloyd's algorithm is often used as an approximation. Here, we start by choosing $k$ random $\vec{\mu}_i$ and determine for all data points which $\vec{\mu}_i$ is closest. The new $\vec{\mu}_i$ are now chosen as the mean of all $\vec{x}_j \in \mathcal{C}_i$. This is repeated untill no data points changes cluster.

Lloyd's algorithm is fast, but can only give convex clusters (draw non-convex example). Also, sometimes a mean is not well-defined.

\subsection{$K$-medoids}

Similar to $K$-means, but here a representative datapoint is chosen as cluster center. Also, the distance is the $L_1$-norm (Manhattan distance) instead of the Euclidian distance in $K$-means (why?).

\subsection{Expectation Maximization Clustering}
Unlike $K$-means and $K$-medoids, EM clustering is a \emph{soft clustering} method. A probability for belonging to a certain cluster is assigned to each data point, instead of a hard label.

In EM clustering we find $n$ clusters represented by multivariate normal distributions.
\begin{equation}
  \label{EM clustering f}
  f_i(\vec{x}|\vec{\mu}_i,\vec{\Sigma}_i) = \frac{1}{(2\pi)^d |\vec{\Sigma}_i|^{\frac{1}{2}}}  \exp \left[ - \frac{(\vec{x}-\vec{\mu}_i)^T\vec{\Sigma}_i^{-1}(\vec{x}-\vec{\mu}_i)}{2} \right]. 
\end{equation}
The assumption is the $x$ is generated by a Gaussian mixture model probability distribution defined by
\begin{equation}
  \label{gaussian mixture model}
f(\vec{x}) = \sum_i f_i(\vec{x}) P(\mathcal{C}_i),
\end{equation}
where the mixture parameters satisfies $\sum_i P(\mathcal{C}_i) = 1$. The model parameters thus include $\theta = \{ \vec{\mu},\vec{\Sigma},P(\mathcal{C}_i)\}$. This goal is to find the set of parameters, such that the probability of observing the dataset, $\vec{D}$, is maximized,
\begin{equation}
  \label{theta max}
  \theta^* = \arg \max_{\theta} \; P(\vec{D}|\theta) =  \arg \max_{\theta} \; \ln P(\vec{D}|\theta).
\end{equation}
Finding $\theta^*$ is generally hard. Again, we can use an iterative algorithm to approximate $\theta^*$ in a two-step process. The first step (\emph{estimation step}) is to calculate $w_{ij} = P(\mathcal{C}_i|\vec{x}_j)$, which is \dots

The second step (\emph{maximization step}) is to reestimate the model parameters using the weights, $w_{ij}$. The means are estimated as
\begin{equation}
  \label{EM means}
  \vec{\mu}_i = \frac{\sum_j w_{ij} \vec{x}_j}{\sum_j w_{ij}}.
\end{equation}
The cluster variance matrix is assumed diagonal for simplicity (otherwise the number of parameters is huge). The covariance matrix is esimated as the weighted covariance for each cluster,
\begin{equation}
  \label{covariance estimate}
  \vec{\Sigma}_i = \frac{\sum_j w_{ij} \vec{z}_{ji}\cdot \vec{z}^T_{ji}}{\vec{w}_i^T\bm{1}},
\end{equation}
where $\vec{z}_{ji} = \vec{x}_j - \vec{\mu}_i$. Finally, the mixture parameters are estimated as
\begin{equation}
  \label{mixture parameter estimation}
  P(\mathcal{C}_i) = \frac{\vec{w}_i^T \vec{1}}{n}.
\end{equation}
These update rules follows from differentiation of the log likelihood function, $\ln P(\vec{D}|\theta)$, with respect to the parameters, and setting it to zero. 


\subsection{Performance measures}


\section{Unsupervised Learning - Outlier Detection and Dimensionality Reduction}


\end{document}